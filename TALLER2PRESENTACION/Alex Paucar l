title: "MODELO DE REGRESIÓN SIMPLE"
author: "Alex Paucar"
date: "12 de abril de 2018"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

##**Introducción**

La regresión lineal simple consiste en generar un modelo de regresión (ecuación de una recta) que permita explicar la relación lineal que existe entre dos variables. A la variable dependiente o respuesta se le identifica como $Y$ y a la variable predictora o independiente como $X$

$Y=\beta_0+\beta_1X_1+\epsilon$

donde:

$\beta_0$ = la ordenada en el origen

$\beta_1$ = la pendiente

$\epsilon$ = error aleatorio

##
En la gran mayoría de casos, los valores $\beta_0$ y $\beta_1$ poblacionales son desconocidos, por lo que, a partir de una muestra, se obtienen sus estimaciones $\beta^0$ y $\beta^1$. Estas estimaciones se conocen como coeficientes de regresión o least square coefficient estimates, ya que toman aquellos valores que minimizan la suma de cuadrados residuales, dando lugar a la recta que pasa más cerca de todos los puntos. (Existen alternativas al método de mínimos cuadrados para obtener las estimaciones de los coeficientes).

$\hat{y}=\hat{\beta_0}-\hat{\beta_1x}$

$\beta_1=\frac{\sum^n_{i=1}(x_i-\hat{x})(y_i-\hat{y})}{\sum^n_{i=1}(x_i-\hat{x})^2}=\frac{Sy}{Sx}R$ 

$\hat{\beta_0}=\bar{y}-\hat{\beta_1}\bar{x}$

##
Donde $Sy$ y $Sx$ son las desviaciones típicas de cada variable y $R$ el coeficiente de correlación

$\beta^0$ es el valor esperado la variable $Y$ cuando $X$ = 0, es decir, la intersección de la recta con el eje $y$. Es un dato necesario para generar la recta, pero en ocasiones, no tiene interpretación práctica (situaciones en las que $X$ no puede adquirir el valor 0).

Una recta de regresión puede emplearse para diferentes propósitos y dependiendo de ellos es necesario satisfacer distintas condiciones. En caso de querer medir la relación lineal entre dos variables, la recta de regresión lo va a indicar de forma directa (ya que calcula la correlación). Sin embargo, en caso de querer predecir el valor de una variable en función de la otra, no solo se necesita calcular la recta, sino que además hay que asegurar que el modelo sea bueno.

#**MODELO DE PRECIOS DE VIVIENDA**#

##**Regresión lineal simple**
Se pretende predecir el valor de la vivienda en función del porcentaje de pobreza de la población. Empleando la función $lm()$ se genera un modelo de regresión lineal por mínimos cuadrados en el que la variable respuesta es medv y el predictor lstat.

Para lo cual se utilizara el $dataset$ Boston del paquete MASS recoge la mediana del valor de la vivienda en 506 áreas residenciales de Boston. Junto con el precio, se han registrado 13 variables adicionales.

```{r warning=FALSE} 
library(MASS)
library(ISLR)
library(psych)
data("Boston")
```


+ crim: ratio de criminalidad per cápita de cada ciudad.
+ zn: Proporción de zonas residenciales con edificaciones de más de 25.000 pies cuadrados.
+ indus: proporción de zona industrializada.

##
+ chas: Si hay río en la ciudad (= 1 si hay río; 0 no hay).
+ nox: Concentración de óxidos de nitrógeno (partes per 10 millón).
+ rm: promedio de habitaciones por vivienda.
+ age: Proporción de viviendas ocupadas por el propietario construidas antes de 1940.
+ dis: Media ponderada de la distancias a cinco centros de empleo de Boston.
+ rad: Índice de accesibilidad a las autopistas radiales.
+ tax: Tasa de impuesto a la propiedad en unidades de $10,000.
+ ptratio: ratio de alumnos/profesor por ciudad.
+ black: 1000(Bk - 0.63)^2 donde Bk es la proporción de gente de color por ciudad.
+ lstat: porcentaje de población en condición de pobreza.

##
```{r eval=FALSE}
Boston$chas <- as.factor(Boston$chas)
summary(Boston)
```
Dado que hay muchas variables, se grafican por grupos de 4, excluyendo las categóricas.

```{r echo=TRUE}
multi.hist(x = Boston[,1:3], dcol = c("blue","red"), dlty = c("dotted", "solid"),
           main = "")
```


##
```{r echo=TRUE}
multi.hist(x = Boston[,5:9], dcol = c("blue","red"), dlty = c("dotted", "solid"),
           main = "")
```


##
```{r echo=TRUE}
multi.hist(x = Boston[,10:14], dcol = c("blue","red"),
           dlty = c("dotted", "solid"), main = "")
```

##

Una vez revisado los gráficos de cada variable procedemos a realiza el modelo planteado

Utlizamos la función **$lm()$** que genera un objeto que almacena toda la información del modelo
```{r echo= TRUE}
modelo_simple <- lm(data = Boston, formula = medv ~ lstat)
```
**Resultados del modelo**
```{r eval=FALSE}
summary(modelo_simple)
```
Residuals:


|  Min   |   1Q  | Median|   3Q  | Max    |
|--------|-------|-------|-------|--------|
|-15.168 |-3.990 | -1.318| 2.034 |24.500  | 

##

|            |Estimate|Std. Error|t value| Max    |
|------------|--------|----------|-------|--------|
|(Intercept) |34.55384| 0.56263  | 64.41 |<2e-16  |
|lstat       |-0.95005|0.03873   |-24.53 |<2e-16  |

![](C:/Users/apaucar/Desktop/2018/deber.png)

Como se puede observa que el p-value del estadístico F es muy pequeño, indicando que al menos uno de los predictores del modelo está significativamente relacionado con la variable respuesta. Al tratarse de un modelo simple, el p-value de estadístico F es el mismo que el p-value del estadístico t del único predictor incluido en el modelo (lstat). La evaluación del modelo en conjunto puede hacerse a partir de los valores RSE o del valor R2 devuelto en el summary.

##
+ Residual standar error (RSE): En promedio, cualquier predicción del modelo se aleja 6.216 unidades del verdadero valor. Teniendo en cuenta que el valor promedio de la variable respuesta medv es de 22.53, RSE es de $\frac{6.216}{22.53}=27\%$
+ $R2$: El predictor lstatus empleado en el modelo es capaz de explicar el $54.44\%$ de la variabilidad observada en el precio de las viviendas.

La ventaja de $R^2$

es que es independiente de la escala en la que se mida la variable respuesta, por lo que su interpretación es más sencilla.

Los dos coeficientes de regresión ($\beta_0$ y $\beta_1$ ) estimados por el modelo son significativos y se pueden interpretar como:

##
+ Intercept($\beta_0$): El valor promedio del precio de la vivienda cuando el *lstatus* es 0 es de 34.5538 unidades.
+ Predictor lstat($\beta_1$): por cada unidad que se incrementa el predictor *lstat* el precio de la vivienda disminuye en promedio 0.9500 unidades.

La estimación de todo coeficiente de regresión tiene asociada un error estándar, por lo tanto todo coeficiente de regresión tiene su correspondiente intervalo de confianza.
```{r echo=TRUE}
confint(modelo_simple, level = 0.95)
```
##
Como era de esperar dado que el *p-value* del predictor lstat ha resultado significativo para un $\infty=0.05$, su intervalo de confianza del 95% no contiene el valor 0

Una vez generado el modelo, es posible predecir el valor de la vivienda sabiendo el estatus de la población en la que se encuentra. Toda predicción tiene asociado un error y por lo tanto un intervalo. Es importante diferenciar entre dos tipos de intervalo:

Intervalo de confianza: Devuelve un intervalo para el valor promedio de todas las viviendas que se encuentren en una población con un determinado porcentaje de pobreza, supóngase *lstat=10*.

##
```{r echo=TRUE}
predict(object = modelo_simple, newdata = data.frame(lstat = c(10)),
        interval = "confidence", level = 0.95)
```

+ Intervalo de predicción: Devuelve un intervalo para el valor esperado de una vivienda en particular que se encuentre en una población con un determinado porcentaje de pobreza.

```{r echo=TRUE}
predict(object = modelo_simple, newdata = data.frame(lstat = c(10)),
        interval = "prediction", level = 0.95)
```


##
Como es de esperar ambos intervalos están centrados en torno al mismo valor. Si bien ambos parecen similares, la diferencia se encuentra en que los intervalos de confianza se aplican al valor promedio que se espera de y para un determinado valor de x, mientras que los intervalos de predicción no se aplican al promedio. Por esta razón los segundos siempre son más amplios que los segundos.

La creación de un modelo de regresión lineal simple suele acompañarse de una representación gráfica superponiendo las observaciones con el modelo. Además de ayudar a la interpretación, es el primer paso para identificar posibles violaciones de las condiciones de la regresión lineal.

##
```{r echo=TRUE}
attach(Boston)
plot(x = lstat, y = medv, main = "medv vs lstat", pch = 20, col = "grey30")
abline(modelo_simple, lwd = 3, col = "red")
```

##
La representación gráfica de las observaciones muestra que la relación entre ambas variables estudiadas no es del todo lineal, lo que apunta a que otro tipo de modelo podría explicar mejor la relación. Aun así la aproximación no es mala.

Una de las mejores formas de confirmar que las condiciones necesarias para un modelo de regresión lineal simple por mínimos cuadrados se cumplen es mediante el estudio de los residuos del modelo.

En R, los residuos se almacenan dentro del modelo bajo el nombre de residuals. R genera automáticamente los gráficos más típicos para la evaluación de los residuos de un modelo

##
```{r echo=TRUE}
par(mfrow = c(2,2))
plot(modelo_simple)
```

##
 Los residuos confirman que los datos no se distribuyen de forma lineal, ni su varianza constante (plot1). Además se observa que la distribución de los residuos no es normal (plot2). Algunas observaciones tienen un residuo estandarizado absoluto mayor de 3 (1.73 si se considera la raíz cuadrada) lo que es indicativo de observación atípica (plot3). Valores de Leverages (hat) mayores que $2.5x((p+1)/n)$

, siendo p el número de predictores y n el número de observaciones, o valores de Cook mayores de 1 se consideran influyentes (plot4). Todo ello reduce en gran medida la robustez de la estimación del error estándar de los coeficientes de correlación estimados y con ello la del modelo es su conjunto.

Otra forma de identificar las observaciones que puedan ser outliers o puntos con alta influencia (leverage) es emplear las funciones rstudent() y hatvalues()

##
```{r echo=TRUE}
plot(x = modelo_simple$fitted.values, y = abs(rstudent(modelo_simple)),
     main = "Absolute studentized residuals vs predicted values", pch = 20,
     col = "grey30")
abline(h = 3, col = "red")
```

##
```{r echo=TRUE}
plot(hatvalues(modelo_simple), main = "Medición de leverage", pch = 20)
# Se añade una línea en el threshold de influencia acorde a la regla
# 2.5x((p+1)/n)
abline(h = 2.5*((dim(modelo_simple$model)[2]-1 + 1)/dim(modelo_simple$model)[1]),
       col = "red")
```

##
 En este caso muchos de los valores parecen posibles outliers o puntos con alta influencia porque los datos realmente no se distribuyen de forma lineal en los extremos.

Modelo

$precio medio vivienda=34,55 ??? 0,95$  $lstat$
